#!/bin/sh

# USAGE: preprocess-dump WIKITAG [STEPS ...]
#
# See usage note below.

# Process options. (No current options.)

#DONT_PERMUTE=no
while true; do
  case "$1" in
    #--no-permute ) DONT_PERMUTE=yes; shift ;;
    -- ) shift; break ;;
    * ) break ;;
  esac
done

if [ -z "$*" ]; then
  cat >&2 <<FOO
Usage: $0 WIKITAG [STEPS ...]

Generate a set of preprocessed data files from a raw Wikipedia dump, as
downloaded from dumps.wikipedia.org.  WIKITAG is something like
'dewiki-20120225'.

This script goes through a series of steps to preprocess a raw Wikipedia
dump (as downloaded from dumps.wikipedia.org) in a set of data files
describing a series of documents by their word counts and coordinates.
Formerly, this data served as the input to TextGrounder, but now an
additional step is needed to convert the data to the corpus format
currently used by TextGrounder.  A higher-level script
'download-preprocess-wiki' can be used to directly download a dump,
preprocess it, and convert it into a TextGrounder corpus.

STEPS specifies which steps of the process to run. The following is the
list of possible steps that can be given, in the order they are normally
executed:

   combined-article-data: Generate permuted combined article-data file
   coord-counts: Generate word-count values for articles with coordinates
   all-counts: Generate word-count values for all articles
   coord-words: Generate actual words for articles with coordinates
   all-words: Generate actual words for all articles

If STEPS is omitted or given as 'wiki', the first two steps given above
will be executed. This is what 'download-preprocess-wiki' does, and runs
the steps necessary to generate the final TextGrounder corpus.

If STEPS is given as 'all', all of the above steps are executed. This
produces some additional files useful for some applications but not the
primary TextGrounder application, and takes a lot of extra time; for this
reason, these extra steps aren't normally run.

See README.preprocess and the comments in the source file for more details.

FOO
  exit 1
fi

dumppref="$1"; shift

# This needs to be set for all subprocesses we call
export WP_VERSION="$dumppref"

steps="$*"
if [ "$steps" = "" ]; then
  steps="wiki"
fi

if [ "$steps" = "all" ]; then
  steps="combined-article-data coord-counts all-counts coord-words all-words"
elif [ "$steps" = "wiki" ]; then
  steps="combined-article-data coord-counts"
fi

# Ensure that all future commands will operate on the permuted dump file
# rather than the unpermuted one. Theoretically this can be omitted and
# the code in 'run-processwiki' will check if the permuted dump file exists
# to determine whether to operate on it, but apparently there's a possible
# race condition in detection, because this detection isn't reliable when
# everything is run from start to finish (probably something to do with
# NFS delays in reporting changes). So forcibly set it to yes.
export USE_PERMUTED=yes
# Split the dump so we can faster afterwards
run-processwiki split-dump

# Now make everything be simultaneous if possible
export NUM_SIMULTANEOUS=20

for step in $steps; do
  echo "Beginning at `date`: run-processwiki $step"
  run-processwiki $step
  echo "Ending at `date`: run-processwiki $step"
done

echo "Removing remaining split files ..."
rm -rf $dumppref-split*
rm -rf foobar.*
echo "Removing remaining split files ... done."

# mv -i *.bz2 *.txt $TG_CORPUS_DIR/wikipedia/$WP_VERSION
# chmod a-w $TG_CORPUS_DIR/wikipedia/$WP_VERSION/*

