#!/bin/sh

# USAGE: preprocess-dump [--no-permute] WIKITAG [STEPS ...]
#
# This script goes through a series of steps to preprocess a raw Wikipedia
# dump (as downloaded from dumps.wikipedia.org) in a set of data files
# describing a series of documents by their word counts and coordinates.
# Formerly, this data served as the input to TextGrounder, but now an
# additional step is needed to convert the data to the corpus format
# currently used by TextGrounder.  A higher-level script
# 'download-preprocess-wiki' can be used to directly download a dump,
# preprocess it, and convert it into a TextGrounder corpus.
#
# See also README.preprocess.

# Process options

DONT_PERMUTE=no
while true; do
  case "$1" in
    --no-permute ) DONT_PERMUTE=yes; shift ;;
    -- ) shift; break ;;
    * ) break ;;
  esac
done

if [ -z "$*" ]; then
  cat >&2 <<FOO
Usage: $0 [--no-permute] WIKITAG

Generate a set of preprocessed data files from a raw Wikipedia dump, as
downloaded from dumps.wikipedia.org.  WIKITAG is something like
'dewiki-20120225'.  See README.preprocess and the comments in the source file
for more details.

FOO
  exit 1
fi

dumppref="$1"; shift

# This needs to be set for all subprocesses we call
export WP_VERSION="$dumppref"

steps="$*"
if [ "$steps" = "" ]; then
  steps="all"
fi

# steps:
#   combined-article-data: Generate permuted combined article-data file
#   coord-counts: Generate word-count values for articles with coordinates
#   all-counts: Generate word-count values for all articles
#   coord-words: Generate actual words for articles with coordinates
#   all-words: Generate actual words for all articles

if [ "$steps" = "all" ]; then
  steps="combined-article-data coord-counts all-counts coord-words all-words"
elif [ "$steps" = "wiki" ]; then
  steps="combined-article-data coord-counts"
fi

if [ "$DONT_PERMUTE" != yes ]; then
  # Generate article-data file from orginal dump
  USE_PERMUTED=no run-processwiki article-data

  # Generate a permuted dump file; all future commands will operate on the
  # permuted dump file, because we will set USE_PERMUTED appropriately.
  run-permute all
fi

# Apparently there's a possible race condition in detection, so forcibly
# use the permuted file.
export USE_PERMUTED=yes
# Split the dump so we can faster afterwards
run-processwiki split-dump

# Now make everything be simultaneous if possible
export NUM_SIMULTANEOUS=8

for step in $steps; do
  echo "Beginning at `date`: run-processwiki $step"
  run-processwiki $step
  echo "Ending at `date`: run-processwiki $step"
done

echo "Removing remaining split files ..."
rm -rf $dumppref-split*
rm -rf foobar.*
echo "Removing remaining split files ... done."

# mv -i *.bz2 *.txt $TG_CORPUS_DIR/wikipedia/$WP_VERSION
# chmod a-w $TG_CORPUS_DIR/wikipedia/$WP_VERSION/*

