This file describes how to preprocess the Wikipedia dump to get the various
necessary files.  The most important script is 'run-processwiki', which
mostly makes use of 'processwiki.py'.

=========== Quick start ==========
Use 'preprocess-dump' to run all of the steps below.
This calls 'run-processwiki'.

=========== Introduction ==========
For 'run-processwiki', output goes to the current directory, and input mostly
comes from the current directory.  There are several steps to run, which
are described below.

The original Wikipedia dump file needs to be in the current directory,
and it's strongly suggested that this script is run in a newly-created
directory, empty save for the dump file (or a symlink to it), with the
dump file marked read-only through 'chmod a-w'.

All files other than the original dump file (and the disambig-id file
mentioned below) are generated by the script.  The original dump file has
a name like enwiki-20100905-pages-articles.xml.bz2; we also generate a permuted
dump file with a name like enwiki-20100905-permuted-pages-articles.xml.bz2.

The disambig-id file comes from $IN_DISAMBIG_ID_FILE, which is located
in $TG_WIKIPEDIA_DIR (the directory where the results of preprocessing
end up getting stored; set 'config-geolocate' in $TEXTGROUNDIR/bin).
The reason for the exception regarding this particular file is that it's
generated not by us but by Wikiprep, which may take several weeks to run.
This file is also not especially important in the scheme of things --
and in fact the relevant data is not currently used at all.  When the
file is present, it lists articles that are identified as "disambiguation"
pages, and this fact goes into one of the fields of the combined article
data file.  If not present, all articles will have "no" in this field.
As just mentioned, no current experiment apps make use of this info.

Other important environment variables (with default settings in
'config-geolocate', but which you might want to override):

WP_VERSION       Specifies which dump file to use, e.g. "enwiki-20100905".
USE_PERMUTED     If set to "no", uses the non-permuted version of the
                 dump file.  If set to "yes", always uses the permuted
                 version.  If unset, attempts to auto-detect the presence of
                 the permuted version, using it if so, otherwise non-permuted.


============== How to do preprocessing from scratch ===============

The following is a possible set of steps to use to generate the necessary
data files from scratch.

1. Create a new directory to work in, where you have a lot of free space.
   (For example, the /scratch dir on Longhorn.) Either download a dump file
   from Wikipedia, or symlink an existing dump file into the new directory.
   Let's say the dump file has the dump prefix 'enwiki-2011007' --
   the English Wikipedia, dump of October 7, 2011.  Also assume that for
   this and all future commands, we're in the new directory.
 
   If we want to download it, we might say

wget http://dumps.wikimedia.org/enwiki/20111007/enwiki-20111007-pages-articles.xml.bz2

   If we want to symlink from somewhere else, we might say

ln -s ../../somewhere/else/enwiki-20111007-pages-articles.xml.bz2 .

2. Generate the basic and combined article data files for the non-permuted dump

WP_VERSION=enwiki-20111007 USE_PERMUTED=no run-processwiki combined-article-data

3. Generate a permuted dump file; all future commands will operate on the
   permuted dump file, because we won't specify a value for USE_PERMUTED.

WP_VERSION=enwiki-20111007 run-permute all

4. Generate the basic and combined article data files for the permuted dump

WP_VERSION=enwiki-20111007 run-processwiki combined-article-data

5. Generate the counts file for articles with coordinates -- this is the info
   needed by most of the Geolocate experiments.

WP_VERSION=enwiki-20111007 run-processwiki coord-counts

6. Generate the counts and words files for all articles, splitting the dump
   file so we can run in parallel.

WP_VERSION=enwiki-20111007 split-dump
WP_VERSION=enwiki-20111007 NUM_SIMULTANEOUS=8 run-processwiki all-counts all-words

7. Move all final generated files (i.e. not including intermediate files) into
   some final directory, e.g. $TG_WIKIPEDIA_DIR.

mv -i *.bz2 *.txt $TG_WIKIPEDIA_DIR
chmod a-w $TG_WIKIPEDIA_DIR/*

   Note the use of '-i', which will query you in case you are trying to
   overwrite an existing while.  We also run 'chmod' afterwards to make all
   the files read-only, to lessen the possibility of accidentally overwriting
   them later in another preprocessing run.

============== How to rerun a single step ===============

If all the preprocessing has already been done for you, and you simply want
to run a single step, then you don't need to do all of the above steps.
However, it's still strongly recommended that you do your work in a fresh
directory, and symlink the dump file into that directory -- in this case the
*permuted* dump file.  We use the permuted dump file for experiments because
the raw dump file has a non-uniform distribution of articles, and so we can't
e.g. count on our splits being uniformly distributed.  Randomly permuting
the dump file and article lists takes care of that.  The permuted dump file
has a name like

enwiki-20111007-permuted-pages-articles.xml.bz2

For example, if want to change processwiki.py to generate bigrams, and then
run it to generate the bigram counts, you might do this:

1. Note that there are currently options `output-coord-counts` to output
   unigram counts only for articles with coordinates (which are the only ones
   needed for standard document geotagging), and `output-all-counts` to
   output unigram counts for all articles.  You want to add corresponding
   options for bigram counts -- either something like
   `output-coord-bigram-counts` and `output-all-bigram-counts`, or an option
   `--n-gram` to specify the N-gram size (1 for unigrams, 2 for bigrams,
   3 for trigrams if that's implemented, etc.).  *DO NOT* in any circumstance
   simply hack the code so that it automatically outputs bigrams instead of
   unigrams -- such code CANNOT be incorporated into the repository, which
   means your mods will become orphaned and unavailable for anyone else.

2. Modify 'config-geolocate' so that it has additional sets of environment
   variables for bigram counts.  For example, after these lines:

COORD_COUNTS_SUFFIX="counts-only-coord-documents.txt"
ALL_COUNTS_SUFFIX="counts-all-documents.txt"

   you'd add

COORD_BIGRAM_COUNTS_SUFFIX="bigram-counts-only-coord-documents.txt"
ALL_BIGRAM_COUNTS_SUFFIX="bigram-counts-all-documents.txt"

   Similarly, after these lines:

OUT_COORD_COUNTS_FILE="$DUMP_PREFIX-$COORD_COUNTS_SUFFIX"
OUT_ALL_COUNTS_FILE="$DUMP_PREFIX-$ALL_COUNTS_SUFFIX"

   you'd add

OUT_COORD_BIGRAM_COUNTS_FILE="$DUMP_PREFIX-$COORD_BIGRAM_COUNTS_SUFFIX"
OUT_ALL_BIGRAM_COUNTS_FILE="$DUMP_PREFIX-$ALL_BIGRAM_COUNTS_SUFFIX"

   And then you'd do the same thing for IN_COORD_COUNTS_FILE and
   IN_ALL_COUNTS_FILE.

3. Modify 'run-processwiki', adding new targets ("steps")
   'coord-bigram-counts' and 'all-bigram-counts'.  Here, you would just
   copy the existing lines for 'coord-counts' and 'all-counts' and modify
   them appropriately.

4. Now finally you can run it:

WP_VERSION=enwiki-20111007 run-processwiki coord-bigram-counts

   This generates the bigram counts for geotagged articles -- the minimum
   necessary for document geotagging.

   Actually, since the above might take awhile and generate a fair amount
   of diagnostic input, you might want to run it in the background
   under nohup, so that it won't die if your terminal connection suddenly
   dies.  One way to do that is to use the TextGrounder 'run-nohup' script:

WP_VERSION=enwiki-20111007 run-nohup --id do-coord-bigram-counts run-processwiki coord-bigram-counts

   Note that the '--id do-coord-bigram-counts' is optional; all it does is
   insert the text "do-coord-bigram-counts" into the file that it stores
   stdout and stderr output into.  This file will have a name beginning
   'run-nohup.' and ending with a timestamp.  The beginning and ending of the
   file will indicate the starting and ending times, so you can see how long
   it took.

   If you want to generate bigram counts for all articles, you could use a
   similar command line, although it might take a couple of days to complete.
   If you're on Longhorn, where you only have 24-hour time slots, you might
   consider using the "divide-and-conquer" mode.  The first thing is to
   split the dump file, like this:

WP_VERSION=enwiki-20111007 run-processwiki split-dump

   This takes maybe 45 mins and splits the whole dump file into 8 pieces.
   (Controllable through NUM_SPLITS.)

   Then, each operation you want to do in divide-and-conquer mode, run it
   by setting NUM_SIMULTANEOUS to something more than 1, e.g.

WP_VERSION=enwiki-20111007 NUM_SIMULTANEOUS=8 run-processwiki all-bigram-counts

   (although you probably want to wrap it in 'run-nohup').  Essentially,
   this runs 8 simultaneous run-processwiki processes (which fits well with
   the workhorse Longhorn machines, since they are 8-core), one on each of
   the 8 splits, and then concatenates the results together at the end.
   You can set a NUM_SIMULTANEOUS that's lower than the number of splits,
   and you get only that much simultaneity.
